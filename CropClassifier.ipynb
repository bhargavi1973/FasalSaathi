{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYx/kCImR8z0pI/exruPVq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargavi1973/FasalSaathi/blob/main/CropClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the file from zip folder\n"
      ],
      "metadata": {
        "id": "BaK6K4rmKXev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/dataset8.zip'\n",
        "extract_path = '/content/dataset8_extracted'\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"'{zip_path}' unzipped to '{extract_path}'\")\n",
        "\n",
        "# Find the CSV file in the extracted directory\n",
        "csv_file = None\n",
        "for root, _, files in os.walk(extract_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            csv_file = os.path.join(root, file)\n",
        "            break\n",
        "    if csv_file:\n",
        "        break\n",
        "\n",
        "if csv_file:\n",
        "    print(f\"Found CSV file: {os.path.basename(csv_file)}\")\n",
        "    # Optionally, update the CSV_PATH in the next cell if necessary\n",
        "    # For this notebook, the path is already set correctly in the next cell\n",
        "    print(f\"New CSV path: {csv_file}\")\n",
        "else:\n",
        "    print(\"No CSV file found in the extracted directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiVw9XdkFK3r",
        "outputId": "e9c0bcda-fc8e-4552-8d31-97eb370252c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/dataset8.zip' unzipped to '/content/dataset8_extracted'\n",
            "Found CSV file: Crop_recommendation.csv\n",
            "New CSV path: /content/dataset8_extracted/Crop_recommendation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the xgboost classifier"
      ],
      "metadata": {
        "id": "ZW0i3iO5Kh7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# yield_predict.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "# Updated path to use the extracted CSV from dataset8.zip\n",
        "CSV_PATH = \"/content/dataset8_extracted/Crop_recommendation.csv\"\n",
        "# Update TARGET_COL based on the new dataset (assuming 'label' or similar, adjust if needed)\n",
        "TARGET_COL = \"label\" # This is the target column for classification\n",
        "RANDOM_STATE = 42"
      ],
      "metadata": {
        "id": "9inIUsryNhAY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Load dataset\n",
        "# -----------------------\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    # Removed the demo dataset creation as we are using a specific file\n",
        "    raise FileNotFoundError(f\"{CSV_PATH} not found.\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "# Update required_cols based on the new dataset (adjust if needed after inspecting the data)\n",
        "required_cols = {'N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall', TARGET_COL}\n",
        "if not required_cols.issubset(set(df.columns)):\n",
        "    # Print existing columns to help identify the correct ones\n",
        "    print(f\"Columns in the new CSV: {df.columns.tolist()}\")\n",
        "    raise ValueError(f\"CSV must contain columns: {required_cols}. Found: {df.columns.tolist()}\")\n"
      ],
      "metadata": {
        "id": "sikbS92tNqBb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -----------------------\n",
        "# Helper: create synthetic demo dataset if none provided\n",
        "# This function is no longer needed as we are using a specific dataset\n",
        "# -----------------------\n",
        "# def create_demo_csv(path):\n",
        "#     np.random.seed(RANDOM_STATE)\n",
        "#     n = 1000\n",
        "#     crops = ['rice', 'maize', 'wheat', 'groundnut']\n",
        "#     df = pd.DataFrame({\n",
        "#         'N': np.random.uniform(10, 180, n),            # kg/ha available or applied\n",
        "#         'P': np.random.uniform(5, 80, n),\n",
        "#         'K': np.random.uniform(5, 120, n),\n",
        "#         'temp': np.random.uniform(18, 35, n),         # mean temp degC\n",
        "#         'humidity': np.random.uniform(40, 95, n),     # %\n",
        "#         'ph': np.random.uniform(4.5, 8.5, n),\n",
        "#         'rainfall': np.random.uniform(100, 1500, n),  # mm over season\n",
        "#         'crop_type': np.random.choice(crops, n)\n",
        "#     })\n",
        "#     # synthetic yield generation (toy function)\n",
        "#     def synthetic_yield(row):\n",
        "#         base = {'rice': 4000, 'maize': 5000, 'wheat': 3500, 'groundnut': 2500}[row['crop_type']]\n",
        "#         # contributions\n",
        "#         n_eff = min(row['N'], 150) * 6    # rough kg/ha per N kg\n",
        "#         p_eff = min(row['P'], 60) * 4\n",
        "#         k_eff = min(row['K'], 80) * 3\n",
        "#         rain_factor = -abs(row['rainfall'] - 800) * 0.5  # penalty if too far from 800\n",
        "#         temp_penalty = -max(0, (row['temp'] - 30)) * 20\n",
        "#         ph_penalty = -abs(row['ph'] - 6.5) * 100\n",
        "#         noise = np.random.normal(0, 200)\n",
        "#         return base + n_eff + p_eff + k_eff + rain_factor + temp_penalty + ph_penalty + noise\n",
        "\n",
        "#     df[TARGET_COL] = df.apply(synthetic_yield, axis=1).clip(lower=200)\n",
        "#     df.to_csv(path, index=False)\n",
        "#     print(f\"Demo dataset written to {path}\")\n",
        "# Basic cleaning\n",
        "df = df.dropna(subset=[TARGET_COL])  # must have target\n",
        "# If there are obvious outliers or bad rows you may want to handle them here\n",
        "\n",
        "# -----------------------\n",
        "# Features & preprocessing\n",
        "# -----------------------\n",
        "# Update feature names based on the new dataset\n",
        "numeric_features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
        "categorical_features = [] # Assuming the new dataset has no categorical features besides the target\n",
        "\n",
        "# Pipelines\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    # for tree models scaling is not necessary; if using linear models add StandardScaler()\n",
        "])\n",
        "\n",
        "# Keep categorical transformer in case needed later, but it's empty for now\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Adjust preprocessor based on the updated feature lists\n",
        "if categorical_features:\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "else:\n",
        "     preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', numeric_transformer, numeric_features)\n",
        "    ])\n",
        "\n",
        "# Encode the target variable since it's categorical (text)\n",
        "label_encoder = LabelEncoder()\n",
        "df[TARGET_COL] = label_encoder.fit_transform(df[TARGET_COL])\n",
        "\n",
        "\n",
        "X = df[numeric_features + categorical_features]\n",
        "y = df[TARGET_COL].values\n",
        "\n",
        "# Create train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=RANDOM_STATE)\n",
        "\n",
        "# Fit preprocessing\n",
        "X_train_trans = preprocessor.fit_transform(X_train)\n",
        "X_test_trans = preprocessor.transform(X_test)\n",
        "\n",
        "# Get feature names for interpretation\n",
        "# Adjust feature name retrieval based on whether categorical features exist\n",
        "if categorical_features:\n",
        "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
        "    # get_feature_names_out returns a numpy array in newer versions\n",
        "    cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
        "    feature_names = numeric_features + cat_names\n",
        "else:\n",
        "    feature_names = numeric_features\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Train XGBoost classifier\n",
        "# Note: We are switching to a classifier as the target is categorical\n",
        "# -----------------------\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=RANDOM_STATE,\n",
        "    use_label_encoder=False, # Deprecated in newer XGBoost, set to False\n",
        "    eval_metric='merror' # Metric for multi-class classification error\n",
        ")\n",
        "\n",
        "# use early stopping on a validation split\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train_trans, y_train, test_size=0.15, random_state=RANDOM_STATE)\n",
        "model.fit(\n",
        "    X_tr, y_tr,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=50\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# Evaluate\n",
        "# Note: These evaluation metrics are for classification\n",
        "# -----------------------\n",
        "y_pred = model.predict(X_test_trans)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Feature importance (gain)\n",
        "# -----------------------\n",
        "try:\n",
        "    importances = model.get_booster().get_score(importance_type='gain')\n",
        "    # map to full feature names (xgboost uses f0..fN)\n",
        "    fmap = {}\n",
        "    for k, v in importances.items():\n",
        "        idx = int(k.replace('f', ''))\n",
        "        fmap[feature_names[idx]] = v\n",
        "    print(\"\\nFeature importances (by gain):\")\n",
        "    for fn, val in sorted(fmap.items(), key=lambda x: -x[1])[:20]:\n",
        "        print(f\"  {fn}: {val:.4f}\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# -----------------------\n",
        "# Plot predicted vs actual (Not applicable for classification, removing)\n",
        "# Note: This plot is for regression. If the new dataset is for classification,\n",
        "# a different visualization (e.g., confusion matrix, classification report) is needed.\n",
        "# -----------------------\n",
        "# plt.figure(figsize=(6,6))\n",
        "# sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)\n",
        "# maxv = max(max(y_test), max(y_pred))\n",
        "# minv = min(min(y_test), min(y_pred))\n",
        "# plt.plot([minv, maxv], [minv, maxv], 'r--', linewidth=1.2)\n",
        "# plt.xlabel('Actual yield (kg/ha)') # Label might need adjustment\n",
        "# plt.ylabel('Predicted yield (kg/ha)') # Label might need adjustment\n",
        "# plt.title('Predicted vs Actual Yield') # Title might need adjustment\n",
        "# plt.grid(alpha=0.3)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Save preprocessing + model\n",
        "# -----------------------\n",
        "os.makedirs('artifacts', exist_ok=True)\n",
        "joblib.dump(preprocessor, 'artifacts/preprocessor.joblib')\n",
        "joblib.dump(model, 'artifacts/xgb_crop_classifier.joblib') # Model name adjusted\n",
        "print(\"Saved preprocessor and model to artifacts/\")\n",
        "\n",
        "# -----------------------\n",
        "# Example: predict for a single farmer input - function definition moved to this cell\n",
        "# -----------------------\n",
        "def predict_crop(N, P, K, temperature, humidity, ph, rainfall):\n",
        "    \"\"\"\n",
        "    Predicts the recommended crop based on environmental conditions.\n",
        "\n",
        "    Args:\n",
        "        N (float): Nitrogen content in the soil.\n",
        "        P (float): Phosphorus content in the soil.\n",
        "        K (float): Potassium content in the soil.\n",
        "        temperature (float): Temperature in Celsius.\n",
        "        humidity (float): Humidity percentage.\n",
        "        ph (float): pH level of the soil.\n",
        "        rainfall (float): Rainfall in mm.\n",
        "\n",
        "    Returns:\n",
        "        str: The recommended crop label, or None if an error occurred.\n",
        "    \"\"\"\n",
        "    # Ensure preprocessor, model, and label_encoder are loaded\n",
        "    try:\n",
        "        preprocessor = joblib.load('artifacts/preprocessor.joblib')\n",
        "        model = joblib.load('artifacts/xgb_crop_classifier.joblib')\n",
        "        # Access label_encoder from the global scope where it was trained\n",
        "        label_encoder = globals().get('label_encoder')\n",
        "        if label_encoder is None:\n",
        "            raise FileNotFoundError(\"label_encoder not found. Please run the cell where the model was trained.\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading artifacts: {e}. Make sure you have run the previous cell to train and save the model.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during loading: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    # Create a DataFrame from the input\n",
        "    input_data = pd.DataFrame([{\n",
        "        'N': N,\n",
        "        'P': P,\n",
        "        'K': K,\n",
        "        'temperature': temperature,\n",
        "        'humidity': humidity,\n",
        "        'ph': ph,\n",
        "        'rainfall': rainfall\n",
        "    }])\n",
        "\n",
        "    # Preprocess the input data\n",
        "    input_data_trans = preprocessor.transform(input_data)\n",
        "\n",
        "    # Make a prediction\n",
        "    prediction = model.predict(input_data_trans)\n",
        "\n",
        "    # Convert the predicted label (numeric) back to the crop name (string)\n",
        "    predicted_crop = label_encoder.inverse_transform(prediction)\n",
        "\n",
        "    return predicted_crop[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "R5ydUuo8EhBF",
        "outputId": "346f4e7e-fe60-4b64-bbe0-22d7b1abcd73"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-merror:0.05694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [09:25:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalidation_0-merror:0.01068\n",
            "[100]\tvalidation_0-merror:0.01068\n",
            "[150]\tvalidation_0-merror:0.01068\n",
            "[200]\tvalidation_0-merror:0.01423\n",
            "[250]\tvalidation_0-merror:0.01423\n",
            "[300]\tvalidation_0-merror:0.01423\n",
            "[350]\tvalidation_0-merror:0.01423\n",
            "[400]\tvalidation_0-merror:0.01423\n",
            "[450]\tvalidation_0-merror:0.01423\n",
            "[500]\tvalidation_0-merror:0.01423\n",
            "[550]\tvalidation_0-merror:0.01779\n",
            "[600]\tvalidation_0-merror:0.01779\n",
            "[650]\tvalidation_0-merror:0.01779\n",
            "[700]\tvalidation_0-merror:0.01779\n",
            "[750]\tvalidation_0-merror:0.01779\n",
            "[800]\tvalidation_0-merror:0.01779\n",
            "[850]\tvalidation_0-merror:0.01779\n",
            "[900]\tvalidation_0-merror:0.01779\n",
            "[950]\tvalidation_0-merror:0.01779\n",
            "[999]\tvalidation_0-merror:0.01779\n",
            "Accuracy: 0.9848\n",
            "\n",
            "Classification Report:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'numpy.int64' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1572389188.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nClassification Report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nConfusion Matrix:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m         \u001b[0mlongest_last_line_heading\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"weighted avg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2724\u001b[0;31m         \u001b[0mname_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongest_last_line_heading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0mhead_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:>{width}s} \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" {:>9}\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m         \u001b[0mlongest_last_line_heading\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"weighted avg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2724\u001b[0;31m         \u001b[0mname_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongest_last_line_heading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0mhead_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:>{width}s} \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" {:>9}\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74f9b7df",
        "outputId": "217e6ec9-ba9d-4a88-d785-b14c89d53ddf"
      },
      "source": [
        "# Example usage:\n",
        "# You can replace these values with input taken from the user\n",
        "n_val = 90\n",
        "p_val = 42\n",
        "k_val = 43\n",
        "temp_val = 20.88\n",
        "humidity_val = 82.0\n",
        "ph_val = 6.5\n",
        "rainfall_val = 200\n",
        "\n",
        "recommended_crop = predict_crop(n_val, p_val, k_val, temp_val, humidity_val, ph_val, rainfall_val)\n",
        "\n",
        "if recommended_crop:\n",
        "    print(f\"\\nBased on the input conditions, the recommended crop is: {recommended_crop}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Based on the input conditions, the recommended crop is: rice\n"
          ]
        }
      ]
    }
  ]
}